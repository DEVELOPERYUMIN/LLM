{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0244a17-e020-45e0-baa2-c0466e45aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q -U torch==2.3.1\n",
    "!pip3 install -q -U transformers==4.44.0\n",
    "!pip3 install -q -U datasets==2.18.0\n",
    "!pip3 install -q -U bitsandbytes==0.42.0\n",
    "!pip3 install -q -U peft==0.9.0\n",
    "!pip3 install -q -U accelerate==0.33.0\n",
    "\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchain-teddynote\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-gpu\n",
    "!pip install pymupdf\n",
    "!pip install pymupdf4llm\n",
    "!pip install pdf2docx\n",
    "!pip install pdfplumber\n",
    "\n",
    "!pip install FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb647a0-af1e-45d4-b0ea-6db6d3b167a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline,\n",
    "                          set_seed,\n",
    ")\n",
    "import transformers\n",
    "from peft import LoraConfig, PeftModel\n",
    "from accelerate.utils import is_mlu_available\n",
    "\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from FlagEmbedding import FlagReranker\n",
    "import re\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import pdfplumber\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
    "from langchain.document_loaders import PDFPlumberLoader, PyMuPDFLoader, PyPDFLoader, UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cabd0-a29d-4072-9ba8-508395c52a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login(\"hf_\")\n",
    "LLAMA_CLOUD_API_KEY= \"llx-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03f623-cb88-4f6b-8e3b-1be4c2b858ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"sh2orc/Llama-3.1-Korean-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer = tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e378db-70b7-4b29-839b-3ec80c223094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_summarys(load_path):\n",
    "  \"\"\"테이블 추출 후 llm을 이용하여 자연어로 정리\"\"\"\n",
    "  with pdfplumber.open(load_path) as pdf:\n",
    "      texts = \"\"\n",
    "      for page in pdf.pages:\n",
    "          table = page.extract_table()\n",
    "          print(page)\n",
    "          if table==None:\n",
    "            continue\n",
    "          # RAG 체인 구성\n",
    "          template = \"\"\"다음 리스트들은 2차원 표 형태의 데이터입니다. 테이블 형태의 정보를 자연어로 표현해주세요. 누락되는 정보가 없도록 답변하세요.\n",
    "\n",
    "          {}\"\"\"\n",
    "          content = template.format(str(table))\n",
    "          messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "          prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "          terminators = [\n",
    "                    pipeline.tokenizer.eos_token_id,\n",
    "                    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "                ]\n",
    "\n",
    "          # 답변 추론\n",
    "          print(f\"Question: {content}\")\n",
    "          #print(messages)\n",
    "          #full_response = llm.invoke(prompt)\n",
    "          outputs = pipeline(prompt,\n",
    "                            max_new_tokens=500,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.2,\n",
    "                            top_k=3,\n",
    "                            top_p=0.95,\n",
    "                            eos_token_id = terminators,\n",
    "                            )\n",
    "          summary = outputs[0][\"generated_text\"][len(prompt):]\n",
    "          print(summary)\n",
    "          texts+=summary#여기까지가 파일로드\n",
    "\n",
    "      splitter = RecursiveCharacterTextSplitter(\n",
    "          chunk_size=512,\n",
    "          chunk_overlap=32\n",
    "      )\n",
    "      chunks = splitter.split_text(texts)\n",
    "      return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd77f9-e2ab-4257-bbb8-0829707fd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_text(text):\n",
    "  \"\"\"추출된 텍스트 정보 깔끔하게 정리하기\"\"\"\n",
    "  llama_docs = re.sub(r\"-{5,}\",\"\\n\", text)\n",
    "  #llama_docs = re.sub(r'\\n', r' ',llama_docs)\n",
    "  llama_docs = re.sub(r\"[-*\\x07\\u20E7\\u20E9\\u0003\\u00B7]\", \"\", llama_docs)\n",
    "  llama_docs = re.sub(r'[\\u2027\\u25CB\\u3139]', '', llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u318d\", \"\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u25A1\", \"\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u007C{2,}\", \"\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u25FE\", \" \", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u3147\", \" \", llama_docs)\n",
    "  llama_docs = re.sub(r\"[\\u3010]\", \"[\", llama_docs)\n",
    "  llama_docs = re.sub(r\"[\\u3011]\", \"]\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u2018\", \"'\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\u2019\", \"'\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\t\", \"\\n\", llama_docs)\n",
    "  llama_docs = re.sub(r\" {3,}\", \"\\n\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\n +\", \"\\n\", llama_docs)\n",
    "  llama_docs = re.sub(r\" \\n \", \"\\n\", llama_docs)\n",
    "  llama_docs = re.sub(r\"\\n\\n\", \"\\n\", llama_docs)\n",
    "  llama_docs = re.sub(r'\\n(.{1})\\n', r'\\1',llama_docs)\n",
    "  return llama_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c76eab-cf17-4e7d-9b64-4802a4c8d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_from_pdf(pdf_path):\n",
    "  \"\"\"테이블 추출하기\"\"\"\n",
    "    tables = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            # Extract tables from the page\n",
    "            page_tables = page.extract_tables()\n",
    "\n",
    "            for table in page_tables:\n",
    "                formatted_table = format_table(table)\n",
    "                tables.append({\n",
    "                    'page': page_num + 1,\n",
    "                    'table': formatted_table\n",
    "                })\n",
    "\n",
    "    return tables\n",
    "\n",
    "def format_table(table):\n",
    "    \"\"\"테이블 포매팅하기\"\"\"\n",
    "    formatted = []\n",
    "    for row in table:\n",
    "        formatted_row = []\n",
    "        for cell in row:\n",
    "            if isinstance(cell, list):\n",
    "                # Nested table case\n",
    "                formatted_row.append(format_table(cell, refine_text))\n",
    "            else:\n",
    "                cell = refine_text(str(cell))\n",
    "                formatted_row.append(cell)\n",
    "        formatted.append(formatted_row)\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107b701-8cff-47af-b6ae-9b588e21fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tables(load_path, save_path):\n",
    "  \"\"\"테이블 pdf에서 지우기\"\"\"\n",
    "    doc = pymupdf.open(load_path)\n",
    "    for page in doc:\n",
    "        for tab in page.find_tables():\n",
    "            # process the content of table 'tab'\n",
    "            page.add_redact_annot(tab.bbox)  # wrap table in a redaction annotation\n",
    "            #page.apply_redactions(0, 2, 0)  # erase all table text\n",
    "    doc.save(save_path)\n",
    "    # do text searches and text extractions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a91ed-776f-4bbc-9e1a-c3c03aab9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_refined(load_path):\n",
    "  \"\"\"Document 객체에서 텍스트 추출하기\"\"\"\n",
    "  md_docs = pymupdf4llm.to_markdown(load_path)\n",
    "  text_refined_temp = refine_text(md_docs)\n",
    "\n",
    "  return text_refined_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a616f-4af7-4686-a373-a2053f22abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_text(load_path, save_path):\n",
    "  table_summarys = extract_table_summarys(load_path)\n",
    "  remove_tables(load_path, save_path)\n",
    "  text_only = get_text_refined(save_path)\n",
    "  return text_only, table_summarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bae2b7-b267-4802-824b-5eb7b4d62f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, chunk_size=512, chunk_overlap=32):\n",
    "    \"\"\"PDF 텍스트 추출 후 chunk 단위로 나누기\"\"\"\n",
    "    text_preprocessed , table_summarized = get_preprocessed_text(file_path, \"check.pdf\")\n",
    "\n",
    "    # 텍스트를 chunk로 분할\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on = headers_to_split_on, strip_headers = False)\n",
    "    md_chunks = md_header_splitter.split_text(text_preprocessed)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(md_chunks)\n",
    "    #테이블 요약 이어붙이기\n",
    "    for summary in table_summarized:\n",
    "        chunks.append(Document(page_content =summary))\n",
    "    for chunk in chunks:\n",
    "      print(chunk.page_content)\n",
    "      print(\"*\"*150)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_vector_db(chunks, embeddings):\n",
    "    \"\"\"FAISS DB 생성\"\"\"\n",
    "\n",
    "    # FAISS DB 생성 및 반환\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "\n",
    "def normalize_path(path):\n",
    "    \"\"\"경로 유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "\n",
    "def process_pdfs_from_dataframe(df, base_directory, model_path=\"intfloat/multilingual-e5-large\"):\n",
    "    \"\"\"딕셔너리에 pdf명을 키로해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "\n",
    "    # 임베딩 모델 설정\n",
    "    model_kwargs = {'device': 'cpu'} # 주의\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_path,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "\n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "\n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "        db = create_vector_db(chunks, embeddings)\n",
    "\n",
    "        #기본 리트리버 생성\n",
    "        base_retriever = db.as_retriever()\n",
    "\n",
    "        #sparse리트리버 생성\n",
    "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
    "\n",
    "        #앙상블 리트리버 생성\n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers = [kiwi_bm25_retriever, base_retriever],\n",
    "            weights = [0.5, 0.5],\n",
    "            search_type = \"mmr\"\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return pdf_databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f550f-e857-4ba7-812e-28773de941b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './data' # Your Base Directory\n",
    "df = pd.read_csv('./test.csv')\n",
    "pdf_databases = process_pdfs_from_dataframe(df, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7dc11e-399d-4352-abf8-ee7fd35ac027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 을 이용한 추론 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bcf091-c9a3-4121-bbfd-54d7047b6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU 메모리 정리\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7a458-9a24-4440-9f88-d054079d83c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_indices(numbers, n):\n",
    "    # 리스트의 인덱스와 값을 함께 저장\n",
    "    indexed_numbers = enumerate(numbers)\n",
    "\n",
    "    # 값을 기준으로 내림차순 정렬\n",
    "    sorted_numbers = sorted(indexed_numbers, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 상위 n개의 인덱스만 추출\n",
    "    top_n_indices = [index for index, value in sorted_numbers[:n]]\n",
    "\n",
    "    return top_n_indices\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "flagreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "def format_docs(question, docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "\n",
    "    reranker = flagreranker\n",
    "    scores = []\n",
    "    n = 5\n",
    "    if n > len(docs):\n",
    "      n = len(docs)\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "      score = reranker.compute_score([question, str(docs[i])], normalize=True)\n",
    "      scores.append(score)\n",
    "      indeces = find_top_n_indices(scores, n)\n",
    "\n",
    "    for index in indeces:\n",
    "        context += docs[index].page_content\n",
    "        context += '\\n'\n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # 정규화된 키로 데이터베이스 검색\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "    # RAG 체인 구성\n",
    "    template = \"\"\"정보의 기준 연도에 유의하여 답변하세요.\n",
    "숫자 단위에 유의하여 답변하세요.\n",
    "정확한 정보에 기반하여 답변하세요.\n",
    "다음 정보를 바탕으로 질문에 답하세요:\n",
    "{}\n",
    "\n",
    "### 질문:\n",
    "{}\n",
    "\n",
    "질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고, 불필요한 설명은 피하며 요구된 정보만 제공하세요.\n",
    "\n",
    "### 답변:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #Reranker 사용하려는데 Chain에 어떻게 적용하는지 몰라서 체인 과정을 직접 구현했습니다.\n",
    "    docs = retriever.invoke(question)\n",
    "    context = format_docs(question, docs)\n",
    "    content = template.format(context, question)\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{content}\"}]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    terminators = [\n",
    "              pipeline.tokenizer.eos_token_id,\n",
    "              pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "          ]\n",
    "\n",
    "    # 답변 추론\n",
    "    print(f\"Question: {question}\")\n",
    "    #print(messages)\n",
    "    #full_response = llm.invoke(prompt)\n",
    "    outputs = pipeline(prompt,\n",
    "                       max_new_tokens=400,\n",
    "                       do_sample=True,\n",
    "                       temperature=0.01,\n",
    "                       top_k=4,\n",
    "                       top_p=0.95,\n",
    "                       eos_token_id = terminators,\n",
    "                       )\n",
    "    full_response = outputs[0][\"generated_text\"][len(prompt):]\n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })\n",
    "    del full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d73b54-effe-4364-8352-2e68c8ea012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(\"/content/drive/MyDrive/Dacon/Dacon_Financial_NLP/llama3.1-ko5.csv\", encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f550938c-a962-4b7f-a666-ee6de34d072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리 정리\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa9773-d7e6-44e3-bc4a-b03fe0a62820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
